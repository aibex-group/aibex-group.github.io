@article{Kocher2019,
 abstract = {In sequence modeling tasks the token order matters, but  this information can be partially lost due to the  discretization of the sequence into data points. In this  paper, we study the imbalance between the way certain token  pairs are included in data points and others are not. We  denote this a token order imbalance (TOI) and we link the  partial sequence information loss to a diminished  performance of the system as a whole, both in text and  speech processing tasks. We then provide a mechanism to  leverage the full token order information—Alleviated TOI—by  iteratively overlapping the token composition of data  points. For recurrent networks, we use prime numbers for  the batch size to avoid redundancies when building batches  from overlapped data points. The proposed method achieved  state of the art performance in both text and speech  related tasks.},
 address = {Hong Kong, China. 2019-11},
 author = {Kocher, Noémien and Scuito, Christian and Tarantino,  Lorenzo and Lazaridis, Alexandros and Fischer, Andreas and  Musat, Claudiu},
 doi = {https://doi.org/10.18653/v1/K19-1083},
 journal = {Proceedings of the 23rd Conference on Computational  Natural Language Learning (CoNLL), 3-4 November 2019, Hong  Kong, China},
 pages = {10 p.},
 title = {Alleviating sequence information loss with data  overlapping and prime batch sizes},
 url = {/research/papers/Kocher2019.pdf},
 year = {2019}
}
