@article{Scius-Bertrand2024,
 abstract = {Classifying scanned documents is a challenging problem  that involves image, layout, and text analysis for document  understanding. Nevertheless, for certain benchmark  datasets, notably RVL-CDIP, the state of the art is closing  in to near-perfect performance when considering hundreds of  thousands of training samples. With the advent of large  language models (LLMs), which are excellent few-shot  learners, the question arises to what extent the document  classification problem can be addressed with only a few  training samples, or even none at all. In this paper, we  investigate this question in the context of zero-shot  prompting and few-shot model fine-tuning, with the aim of  reducing the need for human-annotated training samples as  much as possible.},
 address = {Cham. 2024-12},
 author = {Scius-Bertrand, Anna and Jungo, Michael and Vötglin, Lars  and Spat, Jean-Marc and Fischer, Andreas},
 doi = {https://doi.org/10.1007/978-3-031-78495-8_10},
 journal = {Proceedings of the 27th International Conference, ICPR  2024, 1-5 December 2024, Kolkata, India, Part XIX},
 pages = {15 p.},
 title = {Zero-shot prompting and few-shot fine-tuning : revisiting  document image classification using large language models},
 year = {2024}
}
